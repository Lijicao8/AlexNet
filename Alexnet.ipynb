{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as datas\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 设置GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "epochs = 90\n",
    "batchsize = 128\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "lr_initial = 0.01\n",
    "Image_size = 227\n",
    "num_classes = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexnet\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        # input size should be (b x 3 x 227 x 227)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),# (b x 96 x 55 x 55)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),# section 4.4 (b x 96 x 27 x 27)\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2), #(b x 256 x 27 x 27)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2), #(b x 256 x 13 x 13)\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1), #(b x 384 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=384, out_channels= 384, kernel_size=3, padding=1), #(b x 384 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=384, out_channels= 256, kernel_size=3, padding=1), #(b x 256 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2) #(b x 256 x 6 x 6)\n",
    "        )\n",
    "\n",
    "        # Classifier is just a name for Linear layers\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=(256*6*6), out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        )\n",
    "        self.init_bias() # initial bias\n",
    "    \n",
    "    \n",
    "\n",
    "    def init_bias(self):\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer,nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "        # 对第二个、第四个、第五个卷积层bias设为1\n",
    "        nn.init.constant_(self.net[4].bias, 1.0)\n",
    "        nn.init.constant_(self.net[10].bias, 1.0)\n",
    "        nn.init.constant_(self.net[12].bias, 1.0)\n",
    "\n",
    "        '''for layer in self.Classifier:\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "                nn.init.constant_(layer.bias, 1.0)'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.reshape(-1,256*6*6)\n",
    "        return self.Classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Seed: 13931179872293712972\n"
     ]
    }
   ],
   "source": [
    "seed = torch.initial_seed()\n",
    "print('Use Seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorboardX summary writer created\n"
     ]
    }
   ],
   "source": [
    "tbwriter = SummaryWriter(log_dir='LOGDIR')\n",
    "print('TensorboardX summary writer created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Alex_Net created\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "alexnet = AlexNet().to(device=device)\n",
    "print(alexnet)\n",
    "print('Alex_Net created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义转化图片转换函数 (1 x 24 x 24) to (3 x 227 x 227)\n",
    "def custom_transform(image):\n",
    "    # 将单通道图像转换为三通道\n",
    "    image = image.repeat(3, 1, 1)\n",
    "    # 调整图像大小到 (227, 227)\n",
    "    image = transforms.Resize((227, 227))(image)\n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(custom_transform)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, transform=transform,\n",
    "                                      download=True)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform,\n",
    "                                      download=True)\n",
    "train_dataloader = datas.DataLoader(train_dataset,batch_size=batchsize)\n",
    "test_dataloader = datas.DataLoader(test_dataset,batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created\n",
      "LR Scheduler created\n"
     ]
    }
   ],
   "source": [
    "# 定义优化算法\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=lr_initial, momentum=momentum, weight_decay= weight_decay)\n",
    "print('Optimizer created')\n",
    "\n",
    "# multiply LR by 1 / 10 after every 30 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "print('LR Scheduler created')\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijicao/anaconda3/envs/NNADP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tStep: 10 \tLoss: 6.4151 \tAcc: 12\n",
      "Epoch: 1 \tStep: 20 \tLoss: 3.1762 \tAcc: 21\n",
      "Epoch: 1 \tStep: 30 \tLoss: 3.0151 \tAcc: 18\n",
      "Epoch: 1 \tStep: 40 \tLoss: 2.4541 \tAcc: 14\n",
      "Epoch: 1 \tStep: 50 \tLoss: 2.5537 \tAcc: 11\n",
      "Epoch: 1 \tStep: 60 \tLoss: 2.4018 \tAcc: 13\n",
      "Epoch: 1 \tStep: 70 \tLoss: 2.3810 \tAcc: 11\n",
      "Epoch: 1 \tStep: 80 \tLoss: 2.3532 \tAcc: 14\n",
      "Epoch: 1 \tStep: 90 \tLoss: 2.3415 \tAcc: 10\n",
      "Epoch: 1 \tStep: 100 \tLoss: 2.3309 \tAcc: 7\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: 1.4880896515023778e-06\n",
      "\tnet.0.weight - param_avg: -4.360907041700557e-05\n",
      "\tnet.0.bias - grad_avg: 1.2867454870502115e-06\n",
      "\tnet.0.bias - param_avg: -1.3037976714258548e-05\n",
      "\tnet.4.weight - grad_avg: 3.943101773984381e-07\n",
      "\tnet.4.weight - param_avg: 7.631379048689269e-06\n",
      "\tnet.4.bias - grad_avg: 3.385863601579331e-05\n",
      "\tnet.4.bias - param_avg: 0.9953987002372742\n",
      "\tnet.8.weight - grad_avg: 8.817044545139652e-06\n",
      "\tnet.8.weight - param_avg: 4.499455462791957e-05\n",
      "\tnet.8.bias - grad_avg: 2.0420096916495822e-05\n",
      "\tnet.8.bias - param_avg: 8.739939221413806e-05\n",
      "\tnet.10.weight - grad_avg: 2.9142716812202707e-05\n",
      "\tnet.10.weight - param_avg: -0.0004468667320907116\n",
      "\tnet.10.bias - grad_avg: 8.227265061577782e-05\n",
      "\tnet.10.bias - param_avg: 0.9919195771217346\n",
      "\tnet.12.weight - grad_avg: -6.334723002510145e-05\n",
      "\tnet.12.weight - param_avg: -0.0043275016359984875\n",
      "\tnet.12.bias - grad_avg: -0.0004800377646461129\n",
      "\tnet.12.bias - param_avg: 0.9902629852294922\n",
      "\tClassifier.1.weight - grad_avg: -2.079295654766611e-06\n",
      "\tClassifier.1.weight - param_avg: -0.00026333972346037626\n",
      "\tClassifier.1.bias - grad_avg: -3.652466330095194e-05\n",
      "\tClassifier.1.bias - param_avg: 8.12492726254277e-06\n",
      "\tClassifier.4.weight - grad_avg: -9.041034900292289e-06\n",
      "\tClassifier.4.weight - param_avg: -0.00022620732488576323\n",
      "\tClassifier.4.bias - grad_avg: -0.00011989271297352389\n",
      "\tClassifier.4.bias - param_avg: 0.00038118113297969103\n",
      "\tClassifier.6.weight - grad_avg: -5.675247056156907e-13\n",
      "\tClassifier.6.weight - param_avg: 6.081458195694722e-06\n",
      "\tClassifier.6.bias - grad_avg: -1.3969838306981952e-11\n",
      "\tClassifier.6.bias - param_avg: -0.00033368199365213513\n",
      "Epoch: 1 \tStep: 110 \tLoss: 2.3277 \tAcc: 15\n",
      "Epoch: 1 \tStep: 120 \tLoss: 2.3782 \tAcc: 13\n",
      "Epoch: 1 \tStep: 130 \tLoss: 2.3963 \tAcc: 5\n",
      "Epoch: 1 \tStep: 140 \tLoss: 2.3575 \tAcc: 13\n",
      "Epoch: 1 \tStep: 150 \tLoss: 2.3342 \tAcc: 11\n",
      "Epoch: 1 \tStep: 160 \tLoss: 2.3274 \tAcc: 9\n",
      "Epoch: 1 \tStep: 170 \tLoss: 2.3360 \tAcc: 10\n",
      "Epoch: 1 \tStep: 180 \tLoss: 2.3859 \tAcc: 15\n",
      "Epoch: 1 \tStep: 190 \tLoss: 2.3417 \tAcc: 15\n",
      "Epoch: 1 \tStep: 200 \tLoss: 2.3245 \tAcc: 16\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: -1.146202862400969e-06\n",
      "\tnet.0.weight - param_avg: -4.140357850701548e-05\n",
      "\tnet.0.bias - grad_avg: -1.4482338883681223e-06\n",
      "\tnet.0.bias - param_avg: -9.784370377019513e-06\n",
      "\tnet.4.weight - grad_avg: -1.5254693153110566e-07\n",
      "\tnet.4.weight - param_avg: 7.488050869142171e-06\n",
      "\tnet.4.bias - grad_avg: -1.4098946849117056e-05\n",
      "\tnet.4.bias - param_avg: 0.9904059171676636\n",
      "\tnet.8.weight - grad_avg: -3.362486722835456e-06\n",
      "\tnet.8.weight - param_avg: 3.948049561586231e-05\n",
      "\tnet.8.bias - grad_avg: -8.623975190857891e-06\n",
      "\tnet.8.bias - param_avg: 7.778555300319567e-05\n",
      "\tnet.10.weight - grad_avg: -6.016370207362343e-06\n",
      "\tnet.10.weight - param_avg: -0.00044728300417773426\n",
      "\tnet.10.bias - grad_avg: -1.3735846550844144e-05\n",
      "\tnet.10.bias - param_avg: 0.98695307970047\n",
      "\tnet.12.weight - grad_avg: 1.538110700494144e-05\n",
      "\tnet.12.weight - param_avg: -0.004315131343901157\n",
      "\tnet.12.bias - grad_avg: 0.00013406907964963466\n",
      "\tnet.12.bias - param_avg: 0.9852404594421387\n",
      "\tClassifier.1.weight - grad_avg: 6.851892067061272e-07\n",
      "\tClassifier.1.weight - param_avg: -0.0002610966330394149\n",
      "\tClassifier.1.bias - grad_avg: 1.2999463251617271e-05\n",
      "\tClassifier.1.bias - param_avg: 4.320692823966965e-05\n",
      "\tClassifier.4.weight - grad_avg: 1.1040888239222113e-06\n",
      "\tClassifier.4.weight - param_avg: -0.0002202044561272487\n",
      "\tClassifier.4.bias - grad_avg: 1.0157114957110025e-05\n",
      "\tClassifier.4.bias - param_avg: 0.0004706991894636303\n",
      "\tClassifier.6.weight - grad_avg: 3.8926373517773893e-13\n",
      "\tClassifier.6.weight - param_avg: 6.0510888033604715e-06\n",
      "\tClassifier.6.bias - grad_avg: -1.8626451075975936e-11\n",
      "\tClassifier.6.bias - param_avg: -0.0003320169635117054\n",
      "Epoch: 1 \tStep: 210 \tLoss: 2.3727 \tAcc: 12\n",
      "Epoch: 1 \tStep: 220 \tLoss: 2.3130 \tAcc: 18\n",
      "Epoch: 1 \tStep: 230 \tLoss: 2.3011 \tAcc: 11\n",
      "Epoch: 1 \tStep: 240 \tLoss: 2.3277 \tAcc: 10\n",
      "Epoch: 1 \tStep: 250 \tLoss: 2.3239 \tAcc: 15\n",
      "Epoch: 1 \tStep: 260 \tLoss: 2.3216 \tAcc: 16\n",
      "Epoch: 1 \tStep: 270 \tLoss: 2.3619 \tAcc: 5\n",
      "Epoch: 1 \tStep: 280 \tLoss: 2.3733 \tAcc: 14\n",
      "Epoch: 1 \tStep: 290 \tLoss: 2.3048 \tAcc: 12\n",
      "Epoch: 1 \tStep: 300 \tLoss: 2.3151 \tAcc: 11\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: -4.97585801895184e-07\n",
      "\tnet.0.weight - param_avg: -3.2438569178339094e-05\n",
      "\tnet.0.bias - grad_avg: 2.107896534653264e-06\n",
      "\tnet.0.bias - param_avg: -7.103413281583926e-06\n",
      "\tnet.4.weight - grad_avg: -4.4976630420023866e-08\n",
      "\tnet.4.weight - param_avg: 8.753266229177825e-06\n",
      "\tnet.4.bias - grad_avg: 3.518187077133916e-05\n",
      "\tnet.4.bias - param_avg: 0.985450267791748\n",
      "\tnet.8.weight - grad_avg: 8.155403520504478e-06\n",
      "\tnet.8.weight - param_avg: 3.734145502676256e-05\n",
      "\tnet.8.bias - grad_avg: 1.891423380584456e-05\n",
      "\tnet.8.bias - param_avg: 7.588470907649025e-05\n",
      "\tnet.10.weight - grad_avg: 2.726454840740189e-05\n",
      "\tnet.10.weight - param_avg: -0.00044858589535579085\n",
      "\tnet.10.bias - grad_avg: 9.170920384349301e-05\n",
      "\tnet.10.bias - param_avg: 0.9820032715797424\n",
      "\tnet.12.weight - grad_avg: -4.042496220790781e-05\n",
      "\tnet.12.weight - param_avg: -0.004298786167055368\n",
      "\tnet.12.bias - grad_avg: -0.00037655746564269066\n",
      "\tnet.12.bias - param_avg: 0.980254590511322\n",
      "\tClassifier.1.weight - grad_avg: -1.3144596096026362e-06\n",
      "\tClassifier.1.weight - param_avg: -0.000259758293395862\n",
      "\tClassifier.1.bias - grad_avg: -3.4721801057457924e-05\n",
      "\tClassifier.1.bias - param_avg: 6.292940088314936e-05\n",
      "\tClassifier.4.weight - grad_avg: -4.918098966300022e-06\n",
      "\tClassifier.4.weight - param_avg: -0.00021526720956899226\n",
      "\tClassifier.4.bias - grad_avg: -8.733839786145836e-05\n",
      "\tClassifier.4.bias - param_avg: 0.0005607054918073118\n",
      "\tClassifier.6.weight - grad_avg: 2.0372681948550853e-13\n",
      "\tClassifier.6.weight - param_avg: 6.020901764713926e-06\n",
      "\tClassifier.6.bias - grad_avg: 0.0\n",
      "\tClassifier.6.bias - param_avg: -0.00033036040258593857\n",
      "Epoch: 1 \tStep: 310 \tLoss: 2.3180 \tAcc: 16\n",
      "Epoch: 1 \tStep: 320 \tLoss: 2.2853 \tAcc: 9\n",
      "Epoch: 1 \tStep: 330 \tLoss: 2.3170 \tAcc: 15\n",
      "Epoch: 1 \tStep: 340 \tLoss: 2.2960 \tAcc: 12\n",
      "Epoch: 1 \tStep: 350 \tLoss: 2.3608 \tAcc: 9\n",
      "Epoch: 1 \tStep: 360 \tLoss: 2.3066 \tAcc: 13\n",
      "Epoch: 1 \tStep: 370 \tLoss: 2.3236 \tAcc: 9\n",
      "Epoch: 1 \tStep: 380 \tLoss: 2.3910 \tAcc: 10\n",
      "Epoch: 1 \tStep: 390 \tLoss: 2.3360 \tAcc: 14\n",
      "Epoch: 1 \tStep: 400 \tLoss: 2.3293 \tAcc: 12\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: -1.6748180087233777e-06\n",
      "\tnet.0.weight - param_avg: -1.765008710208349e-05\n",
      "\tnet.0.bias - grad_avg: -3.741567979886895e-07\n",
      "\tnet.0.bias - param_avg: -8.137425538734533e-06\n",
      "\tnet.4.weight - grad_avg: -2.5764640554371e-07\n",
      "\tnet.4.weight - param_avg: 1.0764464605017565e-05\n",
      "\tnet.4.bias - grad_avg: -6.3305396906798705e-06\n",
      "\tnet.4.bias - param_avg: 0.9805122017860413\n",
      "\tnet.8.weight - grad_avg: -1.504152010056714e-06\n",
      "\tnet.8.weight - param_avg: 3.3118009014287964e-05\n",
      "\tnet.8.bias - grad_avg: -3.6038784401171142e-06\n",
      "\tnet.8.bias - param_avg: 6.848888733657077e-05\n",
      "\tnet.10.weight - grad_avg: -8.34794536785921e-06\n",
      "\tnet.10.weight - param_avg: -0.0004502924275584519\n",
      "\tnet.10.bias - grad_avg: -2.8792615921702236e-05\n",
      "\tnet.10.bias - param_avg: 0.9770708680152893\n",
      "\tnet.12.weight - grad_avg: 1.3021870472584851e-05\n",
      "\tnet.12.weight - param_avg: -0.004278150387108326\n",
      "\tnet.12.bias - grad_avg: 0.00012322088878136128\n",
      "\tnet.12.bias - param_avg: 0.9753270745277405\n",
      "\tClassifier.1.weight - grad_avg: 6.075766236790514e-07\n",
      "\tClassifier.1.weight - param_avg: -0.00025837437715381384\n",
      "\tClassifier.1.bias - grad_avg: 1.4424362234422006e-05\n",
      "\tClassifier.1.bias - param_avg: 9.095424320548773e-05\n",
      "\tClassifier.4.weight - grad_avg: -1.9271492419647984e-06\n",
      "\tClassifier.4.weight - param_avg: -0.000212154453038238\n",
      "\tClassifier.4.bias - grad_avg: -3.030650259461254e-05\n",
      "\tClassifier.4.bias - param_avg: 0.000626243359874934\n",
      "\tClassifier.6.weight - grad_avg: -4.102730741945004e-12\n",
      "\tClassifier.6.weight - param_avg: 5.990849786030594e-06\n",
      "\tClassifier.6.bias - grad_avg: -4.097819306103645e-11\n",
      "\tClassifier.6.bias - param_avg: -0.00032871184521354735\n",
      "Epoch: 1 \tStep: 410 \tLoss: 2.3551 \tAcc: 14\n",
      "Epoch: 1 \tStep: 420 \tLoss: 2.3138 \tAcc: 14\n",
      "Epoch: 1 \tStep: 430 \tLoss: 2.3491 \tAcc: 15\n",
      "Epoch: 1 \tStep: 440 \tLoss: 2.3483 \tAcc: 15\n",
      "Epoch: 1 \tStep: 450 \tLoss: 2.3049 \tAcc: 12\n",
      "Epoch: 1 \tStep: 460 \tLoss: 2.2931 \tAcc: 24\n",
      "Epoch: 2 \tStep: 470 \tLoss: 2.3747 \tAcc: 12\n",
      "Epoch: 2 \tStep: 480 \tLoss: 2.3497 \tAcc: 8\n",
      "Epoch: 2 \tStep: 490 \tLoss: 2.3195 \tAcc: 17\n",
      "Epoch: 2 \tStep: 500 \tLoss: 2.3450 \tAcc: 10\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: -3.3592930321901804e-06\n",
      "\tnet.0.weight - param_avg: -3.431397772146738e-06\n",
      "\tnet.0.bias - grad_avg: -2.5928354716597823e-06\n",
      "\tnet.0.bias - param_avg: -1.0681643288990017e-05\n",
      "\tnet.4.weight - grad_avg: -6.321230330286198e-07\n",
      "\tnet.4.weight - param_avg: 1.2470710316847544e-05\n",
      "\tnet.4.bias - grad_avg: -2.8243895940249786e-05\n",
      "\tnet.4.bias - param_avg: 0.9755979776382446\n",
      "\tnet.8.weight - grad_avg: -7.131172424124088e-06\n",
      "\tnet.8.weight - param_avg: 2.8752478101523593e-05\n",
      "\tnet.8.bias - grad_avg: -1.6257645256700926e-05\n",
      "\tnet.8.bias - param_avg: 6.0854847106384113e-05\n",
      "\tnet.10.weight - grad_avg: -2.0794610463781282e-05\n",
      "\tnet.10.weight - param_avg: -0.0004484785022214055\n",
      "\tnet.10.bias - grad_avg: -7.198287494247779e-05\n",
      "\tnet.10.bias - param_avg: 0.9721779227256775\n",
      "\tnet.12.weight - grad_avg: 2.9929544325568713e-05\n",
      "\tnet.12.weight - param_avg: -0.004261324182152748\n",
      "\tnet.12.bias - grad_avg: 0.0002805315307341516\n",
      "\tnet.12.bias - param_avg: 0.970382571220398\n",
      "\tClassifier.1.weight - grad_avg: 1.4015917031429126e-06\n",
      "\tClassifier.1.weight - param_avg: -0.00025747373001649976\n",
      "\tClassifier.1.bias - grad_avg: 4.1534593037795275e-05\n",
      "\tClassifier.1.bias - param_avg: 0.0001165999929071404\n",
      "\tClassifier.4.weight - grad_avg: 4.3668890725712117e-07\n",
      "\tClassifier.4.weight - param_avg: -0.00020941783441230655\n",
      "\tClassifier.4.bias - grad_avg: 6.814285825385014e-06\n",
      "\tClassifier.4.bias - param_avg: 0.0006894767284393311\n",
      "\tClassifier.6.weight - grad_avg: 1.835360205829084e-12\n",
      "\tClassifier.6.weight - param_avg: 5.960952421446564e-06\n",
      "\tClassifier.6.bias - grad_avg: 3.725290215195187e-11\n",
      "\tClassifier.6.bias - param_avg: -0.0003270716406404972\n",
      "Epoch: 2 \tStep: 510 \tLoss: 2.3490 \tAcc: 13\n",
      "Epoch: 2 \tStep: 520 \tLoss: 2.3136 \tAcc: 14\n",
      "Epoch: 2 \tStep: 530 \tLoss: 2.3293 \tAcc: 16\n",
      "Epoch: 2 \tStep: 540 \tLoss: 2.3167 \tAcc: 14\n",
      "Epoch: 2 \tStep: 550 \tLoss: 2.3377 \tAcc: 13\n",
      "Epoch: 2 \tStep: 560 \tLoss: 2.3057 \tAcc: 16\n",
      "Epoch: 2 \tStep: 570 \tLoss: 2.3428 \tAcc: 10\n",
      "Epoch: 2 \tStep: 580 \tLoss: 2.3455 \tAcc: 11\n",
      "Epoch: 2 \tStep: 590 \tLoss: 2.3738 \tAcc: 10\n",
      "Epoch: 2 \tStep: 600 \tLoss: 2.3271 \tAcc: 16\n",
      "**********\n",
      "\tnet.0.weight - grad_avg: -6.2522799453290645e-06\n",
      "\tnet.0.weight - param_avg: 2.5470100808888674e-05\n",
      "\tnet.0.bias - grad_avg: -2.577833356554038e-06\n",
      "\tnet.0.bias - param_avg: -1.806045474950224e-05\n",
      "\tnet.4.weight - grad_avg: -1.1048124406443094e-06\n",
      "\tnet.4.weight - param_avg: 1.683882510405965e-05\n",
      "\tnet.4.bias - grad_avg: -2.7560279704630375e-05\n",
      "\tnet.4.bias - param_avg: 0.9707266092300415\n",
      "\tnet.8.weight - grad_avg: -6.751315140718361e-06\n",
      "\tnet.8.weight - param_avg: 2.8728738470817916e-05\n",
      "\tnet.8.bias - grad_avg: -1.592124863236677e-05\n",
      "\tnet.8.bias - param_avg: 6.18591730017215e-05\n",
      "\tnet.10.weight - grad_avg: -5.0180133257526904e-06\n",
      "\tnet.10.weight - param_avg: -0.0004484649689402431\n",
      "\tnet.10.bias - grad_avg: -5.5172117754409555e-06\n",
      "\tnet.10.bias - param_avg: 0.9673063158988953\n",
      "\tnet.12.weight - grad_avg: 1.1155002539453562e-05\n",
      "\tnet.12.weight - param_avg: -0.004244509153068066\n",
      "\tnet.12.bias - grad_avg: 9.987360681407154e-05\n",
      "\tnet.12.bias - param_avg: 0.9654765725135803\n",
      "\tClassifier.1.weight - grad_avg: 3.2500247471034527e-07\n",
      "\tClassifier.1.weight - param_avg: -0.0002561921428423375\n",
      "\tClassifier.1.bias - grad_avg: 1.1309518413327169e-05\n",
      "\tClassifier.1.bias - param_avg: 0.00013889820547774434\n",
      "\tClassifier.4.weight - grad_avg: 1.5791189298397512e-06\n",
      "\tClassifier.4.weight - param_avg: -0.0002067508321488276\n",
      "\tClassifier.4.bias - grad_avg: 3.0462504582828842e-05\n",
      "\tClassifier.4.bias - param_avg: 0.0007414466235786676\n",
      "\tClassifier.6.weight - grad_avg: 4.001776666116841e-13\n",
      "\tClassifier.6.weight - param_avg: 5.931215582677396e-06\n",
      "\tClassifier.6.bias - grad_avg: 1.6763805621433647e-11\n",
      "\tClassifier.6.bias - param_avg: -0.0003254392941016704\n",
      "Epoch: 2 \tStep: 610 \tLoss: 2.3046 \tAcc: 12\n",
      "Epoch: 2 \tStep: 620 \tLoss: 2.3170 \tAcc: 11\n",
      "Epoch: 2 \tStep: 630 \tLoss: 2.3087 \tAcc: 19\n",
      "Epoch: 2 \tStep: 640 \tLoss: 2.3160 \tAcc: 8\n",
      "Epoch: 2 \tStep: 650 \tLoss: 2.3098 \tAcc: 18\n"
     ]
    }
   ],
   "source": [
    "# 训练开始\n",
    "total_step = 1\n",
    "for i in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    for data in train_dataloader:\n",
    "        images, targets = data\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # 计算损失值\n",
    "        output = alexnet(images)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log the information and add to tensorboard\n",
    "        if total_step % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    _, preds = torch.max(output, 1)\n",
    "                    accuracy = torch.sum(preds == targets)\n",
    "\n",
    "                    print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
    "                        .format(i + 1, total_step, loss.item(), accuracy.item()))\n",
    "                    tbwriter.add_scalar('loss', loss.item(), total_step)\n",
    "                    tbwriter.add_scalar('accuracy', accuracy.item(), total_step)\n",
    "        \n",
    "        if total_step % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    # print and save the grad of the parameters\n",
    "                    # also print and save parameter values\n",
    "                    print('*' * 10)\n",
    "                    for name, parameter in alexnet.named_parameters():\n",
    "                        if parameter.grad is not None:\n",
    "                            avg_grad = torch.mean(parameter.grad)\n",
    "                            print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
    "                            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_step)\n",
    "                            tbwriter.add_histogram('grad/{}'.format(name),\n",
    "                                    parameter.grad.cpu().numpy(), total_step)\n",
    "                        if parameter.data is not None:\n",
    "                            avg_weight = torch.mean(parameter.data)\n",
    "                            print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
    "                            tbwriter.add_histogram('weight/{}'.format(name),\n",
    "                                    parameter.data.cpu().numpy(), total_step)\n",
    "                            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_step)\n",
    "\n",
    "        total_step = total_step+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNADP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
